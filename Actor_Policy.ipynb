{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(\"/Users/konstantinosdalampekis/VS_code/Quantum_Challenge/Quantum-Fraud-Detection-/Datasets/PS_20174392719_1491204439457_log.csv\")\n",
    "\n",
    "# Encode the 'type' column\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "type_encoded = encoder.fit_transform(data[['type']])\n",
    "type_encoded_columns = [f\"type_{cat}\" for cat in encoder.categories_[0]]\n",
    "\n",
    "# Create derived features\n",
    "data['balance_diff_org'] = data['oldbalanceOrg'] - data['newbalanceOrig']\n",
    "data['balance_diff_dest'] = data['oldbalanceDest'] - data['newbalanceDest']\n",
    "data['amount_to_balance_ratio'] = data['amount'] / (data['oldbalanceOrg'] + 1e-5)\n",
    "\n",
    "# Combine features into the final dataset\n",
    "final_features = ['step', 'amount', 'oldbalanceOrg', 'newbalanceOrig', \n",
    "                  'oldbalanceDest', 'newbalanceDest', 'balance_diff_org', \n",
    "                  'balance_diff_dest', 'amount_to_balance_ratio', 'isFraud']\n",
    "data = pd.concat([data[final_features], pd.DataFrame(type_encoded, columns=type_encoded_columns)], axis=1)\n",
    "data['isFraud'] = data['isFraud']  # Include target column\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "data[final_features + type_encoded_columns] = scaler.fit_transform(data[final_features + type_encoded_columns])\n",
    "\n",
    "# Display the processed dataset\n",
    "print(data.head())\n",
    "\n",
    "# Add sequential features\n",
    "n = 5  # Number of previous transactions to consider\n",
    "\n",
    "# Create cumulative statistics\n",
    "data['cumulative_amount_last_n'] = data['amount'].rolling(window=n, min_periods=1).sum()\n",
    "data['transaction_count_last_n'] = data['amount'].rolling(window=n, min_periods=1).count()\n",
    "\n",
    "# Add high suspicion flag\n",
    "data['high_suspicion_flag'] = (\n",
    "    (data['amount_to_balance_ratio'] > 0.9) | \n",
    "    (data['balance_diff_org'] < -10000)\n",
    ").astype(int)\n",
    "\n",
    "# Remove redundant features\n",
    "features_to_keep = [\n",
    "    'step', 'amount', 'balance_diff_org', 'balance_diff_dest',\n",
    "    'amount_to_balance_ratio', 'cumulative_amount_last_n',\n",
    "    'transaction_count_last_n', 'high_suspicion_flag'\n",
    "] + list(type_encoded_columns)\n",
    "\n",
    "data = data[features_to_keep + ['isFraud']]  # Retain target column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "class FraudDetectionEnv(Env):\n",
    "    def __init__(self, data, max_steps=100):\n",
    "        \"\"\"\n",
    "        Initialize the Fraud Detection Environment.\n",
    "        :param data: Preprocessed transaction dataset.\n",
    "        :param max_steps: Number of steps in each episode.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.features = data.columns.drop(['isFraud'])  # All features except target\n",
    "        self.action_space = Discrete(2)  # Actions: {0: Non-Fraud, 1: Fraud}\n",
    "        self.observation_space = Box(\n",
    "            low=-np.inf, high=np.inf, shape=(len(self.features),), dtype=np.float32\n",
    "        )\n",
    "        self.max_steps = max_steps\n",
    "        self.current_step = 0\n",
    "        self.episode_rewards = 0\n",
    "        self.current_index = 0\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment to start a new episode.\n",
    "        :return: Initial state (features of the first transaction).\n",
    "        \"\"\"\n",
    "        self.current_step = 0\n",
    "        self.episode_rewards = 0\n",
    "        self.current_index = np.random.randint(0, len(self.data) - self.max_steps)\n",
    "        return self.data[self.features].iloc[self.current_index].values\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Perform an action and advance to the next transaction.\n",
    "        :param action: 0 (Non-Fraud) or 1 (Fraud).\n",
    "        :return: Tuple (next_state, reward, done, info).\n",
    "        \"\"\"\n",
    "        row = self.data.iloc[self.current_index]\n",
    "        is_fraud = row['isFraud']\n",
    "\n",
    "        # Reward System\n",
    "        if action == 1 and is_fraud == 1:  # Correct fraud detection (True Positive)\n",
    "            reward = 2.0  # Increased reward for detecting fraud\n",
    "        elif action == 1 and is_fraud == 0:  # False Positive\n",
    "            reward = -0.5  # Slightly increased penalty for flagging legitimate\n",
    "        elif action == 0 and is_fraud == 1:  # False Negative\n",
    "            reward = -2.5  # Heavier penalty for missing fraud\n",
    "        elif action == 0 and is_fraud == 0:  # True Negative\n",
    "            reward = 0.5  # Unchanged\n",
    "\n",
    "        # Bonus reward for suspicious patterns\n",
    "        if action == 1 and (row['amount_to_balance_ratio'] > 0.9 or row['balance_diff_org'] < -5000):\n",
    "            reward += 0.2\n",
    "\n",
    "        # Update environment state\n",
    "        self.episode_rewards += reward\n",
    "        self.current_step += 1\n",
    "        self.current_index += 1\n",
    "        done = self.current_step >= self.max_steps or self.current_index >= len(self.data) - 1\n",
    "\n",
    "        # Get the next state or mark the episode as done\n",
    "        next_state = None if done else self.data[self.features].iloc[self.current_index].values\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Render the current episode summary.\n",
    "        \"\"\"\n",
    "        print(f\"Step: {self.current_step}, Total Rewards: {self.episode_rewards}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop with Greedy Decay e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hybrid QNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hybrid Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import numpy as np\n",
    "\n",
    "# # class HybridActor(nn.Module):\n",
    "# #     def __init__(self, num_qubits, num_layers, num_actions):\n",
    "# #         super(HybridActor, self).__init__()\n",
    "# #         self.q_weights = nn.Parameter(torch.randn(num_layers, num_qubits))\n",
    "# #         self.fc = nn.Linear(num_qubits, num_actions)\n",
    "\n",
    "# #     def forward(self, x):\n",
    "# #         # Quantum layer: Variational Quantum Circuit (VQC)\n",
    "# #         q_out = variational_circuit(x, self.q_weights.detach().numpy())\n",
    "# #         q_out = torch.tensor(q_out, dtype=torch.float32)\n",
    "        \n",
    "# #         # Classical layer: Fully connected\n",
    "# #         return torch.softmax(self.fc(q_out), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define Actor-Critic Networks\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim, num_actions):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, 128)\n",
    "        self.out = nn.Linear(128, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc(x))\n",
    "        action_probs = torch.softmax(self.out(x), dim=-1)\n",
    "        return action_probs\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, 128)\n",
    "        self.out = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc(x))\n",
    "        value = self.out(x)\n",
    "        return value\n",
    "\n",
    "\n",
    "# Initialize Environment, Actor, and Critic\n",
    "env = FraudDetectionEnv(data, max_steps=100)\n",
    "input_dim = len(env.features)\n",
    "num_actions = env.action_space.n\n",
    "actor = Actor(input_dim, num_actions)\n",
    "critic = Critic(input_dim)\n",
    "\n",
    "# Define Optimizers\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=0.01)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=0.01)\n",
    "\n",
    "# Training Parameters\n",
    "num_episodes = 500\n",
    "gamma = 0.99  # Discount factor\n",
    "\n",
    "# Epsilon-Greedy Parameters\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.1\n",
    "epsilon_decay = 0.995\n",
    "epsilon = epsilon_start\n",
    "\n",
    "# Training Loop\n",
    "reward_history = []\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_rewards = []\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "\n",
    "    while not done:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "        \n",
    "        # Epsilon-Greedy Action Selection\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()  # Random action (exploration)\n",
    "        else:\n",
    "            action_probs = actor(state_tensor)\n",
    "            action_dist = torch.distributions.Categorical(action_probs)\n",
    "            action = action_dist.sample()\n",
    "            log_prob = action_dist.log_prob(action)\n",
    "            log_probs.append(log_prob)\n",
    "\n",
    "        # Critic: Get state value\n",
    "        value = critic(state_tensor)\n",
    "        values.append(value)\n",
    "\n",
    "        # Step environment\n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "        rewards.append(reward)\n",
    "        state = next_state if not done else None\n",
    "\n",
    "    # Decay Epsilon\n",
    "    epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "\n",
    "    # Compute Returns (Discounted Rewards)\n",
    "    returns = []\n",
    "    G = 0\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "    # Convert values to tensor\n",
    "    values = torch.cat(values).squeeze()\n",
    "    log_probs = torch.stack(log_probs)\n",
    "\n",
    "    # Normalize Returns\n",
    "    returns = (returns - returns.mean()) / (returns.std() + 1e-5)\n",
    "\n",
    "    # Compute Advantages\n",
    "    advantages = returns - values.detach()\n",
    "\n",
    "    # Actor Loss (Policy Gradient)\n",
    "    actor_loss = -(log_probs * advantages).mean()\n",
    "\n",
    "    # Critic Loss (TD Error)\n",
    "    critic_loss = advantages.pow(2).mean()\n",
    "\n",
    "    # Optimize Actor and Critic\n",
    "    actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    actor_optimizer.step()\n",
    "\n",
    "    critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    critic_optimizer.step()\n",
    "\n",
    "    # Track total rewards\n",
    "    total_reward = sum(rewards)\n",
    "    reward_history.append(total_reward)\n",
    "\n",
    "    # Logging\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode}: Total Reward = {total_reward}, Epsilon = {epsilon:.4f}\")\n",
    "\n",
    "# Plot Training Rewards\n",
    "plt.plot(reward_history)\n",
    "plt.title(\"Total Rewards Over Episodes\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluation Metrics for Fraud Detection**\n",
    "\n",
    "1. **Accuracy**:  \n",
    "   Overall correctness of predictions.  \n",
    "   \\[\n",
    "   \\text{Accuracy} = \\frac{\\text{True Positives} + \\text{True Negatives}}{\\text{Total Predictions}}\n",
    "   \\]\n",
    "\n",
    "2. **Precision**:  \n",
    "   Proportion of correctly predicted fraud cases among all predicted fraud cases.  \n",
    "   \\[\n",
    "   \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "   \\]\n",
    "\n",
    "3. **Recall (Sensitivity)**:  \n",
    "   Proportion of correctly predicted fraud cases out of all actual fraud cases.  \n",
    "   \\[\n",
    "   \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "   \\]\n",
    "\n",
    "4. **F1-Score**:  \n",
    "   Harmonic mean of precision and recall.  \n",
    "   \\[\n",
    "   \\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "   \\]\n",
    "\n",
    "5. **AUC-ROC**:  \n",
    "   Measures the trade-off between true positive rate and false positive rate.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_agent(actor, env, num_episodes=100):\n",
    "    \"\"\"\n",
    "    Evaluate the trained Actor-Critic agent on the environment.\n",
    "    :param actor: Trained Actor network.\n",
    "    :param env: FraudDetectionEnv environment.\n",
    "    :param num_episodes: Number of episodes for evaluation.\n",
    "    :return: Metrics and visualization data.\n",
    "    \"\"\"\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "            action_probs = actor(state_tensor)\n",
    "            action = torch.argmax(action_probs).item()\n",
    "\n",
    "            # Store true label and prediction\n",
    "            y_true.append(env.data.iloc[env.current_index]['isFraud'])\n",
    "            y_pred.append(action)\n",
    "\n",
    "            # Step the environment\n",
    "            state, _, done, _ = env.step(action)\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc_roc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    metrics = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'AUC-ROC': auc_roc\n",
    "    }\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Non-Fraud\", \"Fraud\"], yticklabels=[\"Non-Fraud\", \"Fraud\"])\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "\n",
    "    # Print Metrics\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    return metrics, y_true, y_pred\n",
    "\n",
    "# Evaluate the trained agent\n",
    "metrics, y_true, y_pred = evaluate_agent(actor, env)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
